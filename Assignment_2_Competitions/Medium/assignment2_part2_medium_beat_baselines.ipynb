{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Machine Learning Course \n",
    "Author: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). Edited by Sergey Kolchenko (@KolchenkoSergey). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help to throw away all HTML tags from an article content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs= True\n",
    "        self.fed = []\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)\n",
    "\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supplementary function to read a JSON line without crashing on escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_line(line=None):\n",
    "    result = None\n",
    "    try:        \n",
    "        result = json.loads(line)\n",
    "    except Exception as e:      \n",
    "        # Find the offending character index:\n",
    "        idx_to_replace = int(str(e).split(' ')[-1].replace(')',''))      \n",
    "        # Remove the offending character:\n",
    "        new_line = list(line)\n",
    "        new_line[idx_to_replace] = ' '\n",
    "        new_line = ''.join(new_line)     \n",
    "        return read_json_line(line=new_line)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features `content`, `published`, `title` and `author`, write them to separate files for train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Олег\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Олег\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Олег\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_and_write(path_to_data, inp_filename, is_train=True):\n",
    "    \n",
    "    features = ['content', 'published', 'title', 'author', 'desc', 'time_read']\n",
    "    prefix = 'train' if is_train else 'test'\n",
    "    feature_files = [open(os.path.join(path_to_data, '{}_{}.txt'.format(prefix, feat)), 'w', encoding='utf-8')\n",
    "                         for feat in features]\n",
    "    text_feature_file = open('{}_{}.txt'.format(prefix, 'text_feature'), 'w', encoding='utf-8')\n",
    "    \n",
    "    with open(os.path.join(path_to_data, inp_filename), encoding='utf-8') as inp_json_file:\n",
    "        for line in tqdm_notebook(inp_json_file):\n",
    "            json_data = read_json_line(line)\n",
    "            \n",
    "            #CONTENT\n",
    "            html = json_data['content'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            content = strip_tags(html)\n",
    "            images = html.count('<img')\n",
    "            paragraphs = html.count('</p>')\n",
    "            links = html.count('<a href=')\n",
    "            length = len(content)\n",
    "            text_feature_file.write('{},{},{},{}\\n'.format(images, paragraphs, links, length))\n",
    "            \n",
    "            #PUBLISHED\n",
    "            published = json_data['published']['$date']\n",
    "            \n",
    "            #TITLE\n",
    "            title = json_data['title'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            \n",
    "            #AUTHOR NAME\n",
    "            author = str(json_data['author']['url'])\n",
    "            \n",
    "            #DESCRIPTION\n",
    "            desc = json_data['meta_tags']['description'].replace('\\n', ' ').replace('\\r', ' ')\n",
    "            if (desc == ''):\n",
    "                desc = 'none'\n",
    "            #word_tokens = word_tokenize(desc)\n",
    "            #desc = \" \".join([ps.stem(w) for w in word_tokens if not w in stop_words and w.isalpha()])\n",
    "            \n",
    "            #TIME TO READ\n",
    "            time = json_data['meta_tags']['twitter:data1'].replace(' ', '')\n",
    "            if (time == ''):\n",
    "                time = '7minread'\n",
    "            \n",
    "            feature_files[0].write(content + '\\n')\n",
    "            feature_files[1].write(published + '\\n')\n",
    "            feature_files[2].write(title + '\\n')\n",
    "            feature_files[3].write(author + '\\n')  \n",
    "            feature_files[4].write(desc + '\\n')  \n",
    "            feature_files[5].write(time + '\\n')  \n",
    "    \n",
    "    for file in feature_files:\n",
    "        file.close()\n",
    "    text_feature_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab7d16d7d2f4a5da89d6f525e049740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_features_and_write('', 'data/train.json', is_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a228e4a50226464ab084b2b361485264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "extract_features_and_write('', 'data/test.json', is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fedff255d1fd4bf58b63d1e7949c14df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1761b243ed84a0e9862190a3c89005d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = open('data/train_author.txt', 'w', encoding='utf-8')\n",
    "with open(os.path.join('data/train.json'), encoding='utf-8') as inp_json_file:\n",
    "    for line in tqdm_notebook(inp_json_file):\n",
    "        json_data = read_json_line(line)\n",
    "        file.write(json_data['author']['url'].split('@')[1] + '\\n')\n",
    "file.close()\n",
    "file = open('data/test_author.txt', 'w', encoding='utf-8')\n",
    "with open(os.path.join('data/test.json'), encoding='utf-8') as inp_json_file:\n",
    "    for line in tqdm_notebook(inp_json_file):\n",
    "        json_data = read_json_line(line)\n",
    "        file.write(json_data['author']['url'].split('@')[1] + '\\n')\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the following groups of features:**\n",
    "    - Tf-Idf with article content (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Tf-Idf with article titles (ngram_range=(1, 2), max_features=100000 but you can try adding more)\n",
    "    - Time features: publication hour, whether it's morning, day, night, whether it's a weekend\n",
    "    - Bag of authors (i.e. One-Hot-Encoded author names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    \n",
    "    def __init__(self, stemmer=PorterStemmer, lemmatizer=WordNetLemmatizer):\n",
    "        self.stemmer = stemmer()\n",
    "        self.lemmatizer = lemmatizer()\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        from string import punctuation, digits\n",
    "        other_unicode_chars = '’’”“\\u200b'\n",
    "        chars_to_remove = ''.join((punctuation,\n",
    "                                   digits,\n",
    "                                   other_unicode_chars))\n",
    "        transtab = str.maketrans(chars_to_remove, ' '*len(chars_to_remove))\n",
    "        return [self.stemmer.stem(self.lemmatizer.lemmatize(token, pos='v')) \n",
    "                for token in word_tokenize(doc.translate(transtab)) \n",
    "                if len(token) >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "temp = []\n",
    "s = Tokenizer()\n",
    "for eggs in stop_words:\n",
    "    token = s(eggs)\n",
    "    if token:\n",
    "        temp += token\n",
    "stop_words = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding titles and content bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62313, 50000)\n",
      "(34645, 50000)\n",
      "(62313, 100000)\n",
      "(34645, 100000)\n",
      "Wall time: 5min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), max_features=50000)\n",
    "X_train_title_sparse = tfidf.fit_transform(open('train_title.txt', encoding='utf-8'))\n",
    "print(X_train_title_sparse.shape)\n",
    "X_test_title_sparse = tfidf.transform(open('test_title.txt', encoding='utf-8'))\n",
    "print(X_test_title_sparse.shape)\n",
    "\n",
    "vect = CountVectorizer(max_features=100000)\n",
    "X_train_content_sparse = vect.fit_transform(open('train_content.txt', encoding='utf-8'))\n",
    "print(X_train_content_sparse.shape)\n",
    "X_test_content_sparse = vect.transform(open('test_content.txt', encoding='utf-8'))\n",
    "print(X_test_content_sparse.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding desc and time_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62313, 50000)\n",
      "(34645, 50000)\n",
      "(62313, 112)\n",
      "(34645, 112)\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 2), tokenizer=Tokenizer(),\n",
    "                                     stop_words=stop_words, max_features=50000)\n",
    "X_train_desc_sparse = tfidf.fit_transform(open('train_desc.txt', encoding='utf-8'))\n",
    "print(X_train_desc_sparse.shape)\n",
    "X_test_desc_sparse = tfidf.transform(open('test_desc.txt', encoding='utf-8'))\n",
    "print(X_test_desc_sparse.shape)\n",
    "\n",
    "vect = CountVectorizer(max_features=50000)\n",
    "X_train_time_read_sparse = vect.fit_transform(open('train_time_read.txt', encoding='utf-8'))\n",
    "print(X_train_time_read_sparse.shape)\n",
    "X_test_time_read_sparse = vect.transform(open('test_time_read.txt', encoding='utf-8'))\n",
    "print(X_test_time_read_sparse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authors bag-of-names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62313, 32365)\n",
      "(34645, 32365)\n",
      "Wall time: 922 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vect = CountVectorizer(max_features=50000)\n",
    "X_train_author_sparse = vect.fit_transform(open('train_author.txt', encoding='utf-8'))\n",
    "print(X_train_author_sparse.shape)\n",
    "X_test_author_sparse = vect.transform(open('test_author.txt', encoding='utf-8'))\n",
    "print(X_test_author_sparse.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding content features like img and len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_df = pd.read_csv('train_text_feature.txt', header=None, names=['images', 'paragraphs', 'links', 'length'])\n",
    "text_test_df = pd.read_csv('test_text_feature.txt', header=None, names=['images', 'paragraphs', 'links', 'length'])\n",
    "text_df = pd.concat([text_train_df, text_test_df])\n",
    "idx_split = text_train_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_img = StandardScaler().fit_transform(text_df[['images']])\n",
    "scaled_p = StandardScaler().fit_transform(text_df[['paragraphs']])\n",
    "scaled_links = StandardScaler().fit_transform(text_df[['links']])\n",
    "scaled_len = StandardScaler().fit_transform(text_df[['length']])\n",
    "text_df['short'] = text_df['length'].apply(lambda size: 1 if (size < 1350) else -1)\n",
    "text_df['medium'] = text_df['length'].apply(lambda size: 1 if (size >= 1350 and size < 2700) else -1)\n",
    "text_df['long'] = text_df['length'].apply(lambda size: 1 if (size >= 2700 and size < 6750) else -1)\n",
    "text_df['huge'] = text_df['length'].apply(lambda size: 1 if (size >= 6750) else -1)\n",
    "\n",
    "\n",
    "X_text_features = np.hstack([scaled_img, \n",
    "                             scaled_p,\n",
    "                             scaled_links, \n",
    "                             #scaled_len,\n",
    "                             text_df['short'].values.reshape(-1, 1),\n",
    "                             text_df['medium'].values.reshape(-1, 1),\n",
    "                             text_df['long'].values.reshape(-1, 1),\n",
    "                             text_df['huge'].values.reshape(-1, 1)\n",
    "                            ])\n",
    "X_train_text_features = X_text_features[:idx_split, :]\n",
    "X_test_text_features = X_text_features[idx_split:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>published</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-08-13 22:54:53.510000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-08-03 07:44:50.331000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-02-05 13:08:17.410000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-05-06 08:16:30.776000+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-06-04 14:46:25.772000+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         published\n",
       "0 2012-08-13 22:54:53.510000+00:00\n",
       "1 2015-08-03 07:44:50.331000+00:00\n",
       "2 2017-02-05 13:08:17.410000+00:00\n",
       "3 2017-05-06 08:16:30.776000+00:00\n",
       "4 2017-06-04 14:46:25.772000+00:00"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dateutil.parser\n",
    "time_train_df = pd.read_csv('train_published.txt', header=None, names=['published'])\n",
    "time_test_df = pd.read_csv('test_published.txt', header=None, names=['published'])\n",
    "time_df = pd.concat([time_train_df, time_test_df])\n",
    "idx_split = time_train_df.shape[0]\n",
    "\n",
    "time_df['published'] = time_df['published'].apply(lambda s: dateutil.parser.parse(s))\n",
    "time_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770080cb42cd4bee99df7b56da2d7393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "time_df['year'] = time_df['published'].apply(lambda ts: ts.year)\n",
    "scaled_year = StandardScaler().fit_transform(time_df[['year']])\n",
    "time_df['month'] = time_df['published'].apply(lambda ts: ts.month)\n",
    "\n",
    "weekdays = ['day%s' % i for i in range(1, 8)]\n",
    "hours = ['hour%s' % i for i in range(0, 24)]\n",
    "\n",
    "for i, day in enumerate(weekdays):\n",
    "    time_df[day] = time_df['published'].apply(lambda ts: 1 if (ts.weekday() == i) else -1)\n",
    "for i, hour in tqdm_notebook(enumerate(hours)):\n",
    "    time_df[hour] = time_df['published'].apply(lambda ts: 1 if (ts.hour == i) else -1)\n",
    "    \n",
    "time_df['isweekend'] = time_df['published'].apply(lambda ts: 1 if (ts.weekday() >= 6) else -1)\n",
    "time_df['winter'] = time_df['month'].apply(lambda month: 1 if (month <= 2 or month == 12) else -1)\n",
    "time_df['spring'] = time_df['month'].apply(lambda month: 1 if (month >= 3 and month <= 5) else -1)\n",
    "time_df['summer'] = time_df['month'].apply(lambda month: 1 if (month >= 6 and month <= 8) else -1)\n",
    "time_df['autumn'] = time_df['month'].apply(lambda month: 1 if (month >= 9 and month <= 11) else -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_time_features = np.hstack([#scaled_year,\n",
    "                          time_df[hours].values.reshape(-1, 24),\n",
    "                          time_df['winter'].values.reshape(-1, 1),\n",
    "                          time_df['spring'].values.reshape(-1, 1),\n",
    "                          time_df['summer'].values.reshape(-1, 1),\n",
    "                          time_df['autumn'].values.reshape(-1, 1),\n",
    "                          time_df['isweekend'].values.reshape(-1, 1),\n",
    "                         ])\n",
    "X_train_time_features = X_time_features[:idx_split, :]\n",
    "X_test_time_features = X_time_features[idx_split:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Join all sparse matrices.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sparse = hstack([X_train_content_sparse, X_train_title_sparse,\n",
    "                         X_train_author_sparse, \n",
    "                         X_train_time_features,\n",
    "                         X_train_desc_sparse,\n",
    "                         X_train_time_read_sparse,\n",
    "                         X_train_text_features,\n",
    "                        ]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sparse = hstack([X_test_content_sparse, X_test_title_sparse,\n",
    "                        X_test_author_sparse, \n",
    "                        X_test_time_features,\n",
    "                        X_test_desc_sparse,\n",
    "                        X_test_time_read_sparse,\n",
    "                        X_test_text_features\n",
    "                       ]).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((62313, 232513), (34645, 232513))"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sparse.shape, X_test_sparse.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read train target and split data for validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv(os.path.join('', 'data/train_log1p_recommends.csv'), index_col='id')\n",
    "y_train = train_target['log_recommends'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_part_size = int(0.7 * train_target.shape[0])\n",
    "X_train_part_sparse = X_train_sparse[:train_part_size, :]\n",
    "y_train_part = y_train[:train_part_size]\n",
    "X_valid_sparse =  X_train_sparse[train_part_size:, :]\n",
    "y_valid = y_train[train_part_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train a simple Ridge model and check MAE on the validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 172 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge = Ridge(random_state=17, alpha=0.5)\n",
    "ridge.fit(X_train_part_sparse, (y_train_part))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4464968989567717\n",
      "Wall time: 3.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ridge_pred = (ridge.predict(X_valid_sparse))\n",
    "ridge_valid_mae = mean_absolute_error((y_valid), ridge_pred)\n",
    "print(ridge_valid_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x185aec0bb00>"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.hist(np.log1p(y_train_part), bins=30, alpha=.5, color='red', label='train', range=(0,10))\n",
    "plt.hist((y_valid), bins=30, alpha=.5, color='blue', label='valid', range=(0,10))\n",
    "plt.hist((ridge_pred), bins=30, alpha=.5, color='green', label='Ridge', range=(0,10))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the same Ridge with all available data, make predictions for the test set and form a submission file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_train_sparse, (y_train))\n",
    "ridge_test_pred = (ridge.predict(X_test_sparse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_submission_file(prediction, filename, path_to_sample='sample_submission.csv'):\n",
    "    submission = pd.read_csv(path_to_sample, index_col='id')\n",
    "    submission['log_recommends'] = prediction\n",
    "    submission.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_test_target = 4.33328\n",
    "write_submission_file(ridge_test_pred + mean_test_target - y_train.mean(), 'medium_submission_!!!.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ideas for improvement:\n",
    "\n",
    "- Engineer good features, this is the key to success. Some simple features will be based on publication time, authors, content length and so on\n",
    "- You may not ignore HTML and extract some features from there\n",
    "- You'd better experiment with your validation scheme. You should see a correlation between your local improvements and LB score\n",
    "- Try TF-IDF, ngrams, Word2Vec and GloVe embeddings\n",
    "- Try various NLP techniques like stemming and lemmatization\n",
    "- Tune hyperparameters. In our example, we've left only 50k features and used C=1 as a regularization parameter, this can be changed\n",
    "- SGD and Vowpal Wabbit will learn much faster\n",
    "- Play around with blending and/or stacking. An intro is given in [this Kernel](https://www.kaggle.com/kashnitsky/ridge-and-lightgbm-simple-blending) by @yorko \n",
    "- In our course, we don't cover neural nets. But it's not obliged to use GRUs/LSTMs/whatever in this competition.\n",
    "\n",
    "Good luck!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
